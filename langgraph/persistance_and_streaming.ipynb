{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langgraph\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.messages import AnyMessage,HumanMessage,AIMessage,SystemMessage,ToolMessage\n",
    "from typing import TypedDict,List,Annotated\n",
    "import operator\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langgraph.graph import StateGraph,END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPEN_API_KEY2\")\n",
    "os.environ[\"TAVILY_API_KEY\"]=os.getenv(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool = TavilySearchResults(max_results=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    \"\"\" AgentState` is a TypedDict that represents the state of an agent.\n",
    "    # The `messages` key holds a list of `AnyMessage` objects.\n",
    "    # The `Annotated` type is used here to associate the `messages` list with\n",
    "    # an additional metadata operator (`operator.add`), which suggests that new \n",
    "    # messages might be appended using this operator \"\"\"\n",
    "    messages: Annotated[list[AnyMessage], operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = MemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, model, tools, checkpointer, system=\"\"):\n",
    "        \"\"\"\n",
    "        Initializes an Agent with a model, tools, a checkpointer, and an optional system message.\n",
    "\n",
    "        Parameters:\n",
    "        - model: The language model that interacts with the agent (e.g., OpenAI's GPT model).\n",
    "        - tools: A list of tools the agent can use, which will be stored in a dictionary by their name.\n",
    "        - checkpointer: Used to save and restore the state of the graph, ensuring continuity.\n",
    "        - system: An optional system message that can be prepended to the conversation context.\n",
    "\n",
    "        The agent's behavior is modeled as a graph (`StateGraph`), where nodes represent\n",
    "        different functions (`call_openai`, `take_action`), and edges define the flow between them.\n",
    "        The graph starts at the \"llm\" node and moves conditionally based on whether the agent\n",
    "        needs to take an action (e.g., invoke a tool).\n",
    "        \"\"\"\n",
    "        self.system = system\n",
    "        \n",
    "        # Initialize the state graph with the structure of the agent's state (AgentState)\n",
    "        graph = StateGraph(AgentState)\n",
    "        \n",
    "        # Add nodes (functions) to the graph:\n",
    "        # \"llm\" node invokes the OpenAI model and updates the state with a new message.\n",
    "        # \"action\" node handles tool invocation based on the latest message.\n",
    "        graph.add_node(\"llm\", self.call_openai)\n",
    "        graph.add_node(\"action\", self.take_action)\n",
    "        \n",
    "        # Conditionally add edges from \"llm\" to \"action\" based on whether an action is required.\n",
    "        # If the latest message includes tool calls, go to \"action\"; otherwise, end.\n",
    "        graph.add_conditional_edges(\"llm\", self.exists_action, {True: \"action\", False: END})\n",
    "        \n",
    "        # Add an edge from \"action\" back to \"llm\", enabling continuous conversation or actions.\n",
    "        graph.add_edge(\"action\", \"llm\")\n",
    "        \n",
    "        # Set the starting point of the graph as \"llm\".\n",
    "        graph.set_entry_point(\"llm\")\n",
    "        \n",
    "        # Compile the graph with a checkpointer for saving and restoring agent state.\n",
    "        self.graph = graph.compile(checkpointer=checkpointer)\n",
    "        \n",
    "        # Convert tools into a dictionary for easy access during execution (by tool name).\n",
    "        self.tools = {t.name: t for t in tools}\n",
    "        \n",
    "        # Bind the model to the tools so it can call them when necessary.\n",
    "        self.model = model.bind_tools(tools)\n",
    "\n",
    "    def call_openai(self, state: AgentState):\n",
    "        \"\"\"\n",
    "        Calls the OpenAI model with the current message state.\n",
    "\n",
    "        Parameters:\n",
    "        - state: The current state of the agent, containing messages.\n",
    "\n",
    "        If a system message is defined, it will be prepended to the list of messages.\n",
    "        The model is then invoked with the messages, and a new message is returned,\n",
    "        which gets added back to the state.\n",
    "        \"\"\"\n",
    "        # Retrieve messages from the current state.\n",
    "        messages = state['messages']\n",
    "        \n",
    "        # If a system message is provided, prepend it to the messages.\n",
    "        if self.system:\n",
    "            messages = [SystemMessage(content=self.system)] + messages\n",
    "        \n",
    "        # Call the model with the messages and receive a new message.\n",
    "        message = self.model.invoke(messages)\n",
    "        \n",
    "        # Return the updated state with the new message added.\n",
    "        return {'messages': [message]}\n",
    "\n",
    "    def exists_action(self, state: AgentState):\n",
    "        \"\"\"\n",
    "        Checks whether the last message in the state requires an action (tool invocation).\n",
    "\n",
    "        Parameters:\n",
    "        - state: The current state of the agent, containing messages.\n",
    "\n",
    "        Returns:\n",
    "        - True if the last message includes tool calls, False otherwise.\n",
    "        \"\"\"\n",
    "        # Get the latest message in the state.\n",
    "        result = state['messages'][-1]\n",
    "        \n",
    "        # Check if the message contains any tool calls.\n",
    "        return len(result.tool_calls) > 0\n",
    "\n",
    "    def take_action(self, state: AgentState):\n",
    "        \"\"\"\n",
    "        Invokes the necessary tools based on the last message's tool calls.\n",
    "\n",
    "        Parameters:\n",
    "        - state: The current state of the agent, containing messages.\n",
    "\n",
    "        The method loops through the list of tool calls, invokes each tool with its\n",
    "        respective arguments, and appends the result back to the messages in the form\n",
    "        of a ToolMessage.\n",
    "        \"\"\"\n",
    "        # Retrieve tool calls from the latest message in the state.\n",
    "        tool_calls = state['messages'][-1].tool_calls\n",
    "        \n",
    "        # Initialize a list to hold the results of the tool invocations.\n",
    "        results = []\n",
    "        \n",
    "        # Loop through each tool call, invoke the respective tool, and store the result.\n",
    "        for t in tool_calls:\n",
    "            print(f\"Calling: {t}\")\n",
    "            \n",
    "            # Invoke the tool by its name, passing the required arguments.\n",
    "            result = self.tools[t['name']].invoke(t['args'])\n",
    "            \n",
    "            # Create a ToolMessage with the result and add it to the results list.\n",
    "            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n",
    "        \n",
    "        # Log that the tool action is complete and the agent is returning to the model.\n",
    "        print(\"Back to the model!\")\n",
    "        \n",
    "        # Return the updated state with the new tool result messages.\n",
    "        return {'messages': results}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "prompt=\"\"\"You are a smart research assistant. Use the search engine to look up information. \\\n",
    "You are allowed to make multiple calls (either together or in sequence). \\\n",
    "Only look up information when you are sure of what you want. \\\n",
    "If you need to look up some information before asking a follow up question, you are allowed to do that!\n",
    "\"\"\"\n",
    "abot = Agent(model=model,tools=[tool],checkpointer=memory,system=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"What is the weather in sf?\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=abot.graph.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"i also mention sf but why you guess karachi\")]},\n",
    "    config={\"configurable\": {\"thread_id\": 42}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I apologize for the confusion. You mentioned both San Francisco (sf) and Karachi in your queries. I associated Karachi with your location based on the context of your questions. If you have any specific questions or topics you'd like to discuss, feel free to let me know!\""
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "78a73e9cea53431a004a8cb57099f417374d636685740d6545615ecc2fea032b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
